{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4119 entries, 0 to 4118\n",
      "Data columns (total 7 columns):\n",
      "Department:          4113 non-null object\n",
      "Position Title:      4112 non-null object\n",
      "POSITION SUMMARY:    3458 non-null object\n",
      "responsabilities     3815 non-null object\n",
      "EI                   2763 non-null object\n",
      "CA                   3142 non-null object\n",
      "TS                   3385 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 225.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import textblob\n",
    "from tqdm import tqdm\n",
    "df=pd.read_csv('total_profiles.csv')\n",
    "df = df[['Department: ','Position Title:','POSITION SUMMARY:','responsabilities','EI','CA','TS']]\n",
    "\n",
    "#df.to_csv('total_profiles_selected.csv')\n",
    "#df.fillna(\"\", inplace=True)\n",
    "\n",
    "#df=df.dropna()\n",
    "\n",
    "#df['Department: '].apply(lambda txt: ''.join(textblob.TextBlob(txt).correct()))\n",
    "#[df[i].apply(lambda txt: ''.join(textblob.TextBlob(txt).correct())) for i in s]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rushi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rushi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\users\\rushi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "\n",
    "from textblob import Word\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 4119\n",
      "Total Length after removing NA: 4119\n",
      "unique Length: 3551\n",
      "max word count 16\n",
      "min word count 1\n",
      "max word count 5.6904588492352515\n",
      "average character count 123\n",
      "min character count 0\n",
      "Total Stop Words 384\n",
      "Total special_character 1075\n",
      "Total numbers 99\n",
      "Total upprfamilyposse 757\n",
      "\n",
      "\n",
      "Common words and count:\n",
      " analyst        506\n",
      "student        486\n",
      "engineer       468\n",
      "technical      371\n",
      "business       367\n",
      "oil            366\n",
      "engineering    350\n",
      "operations     334\n",
      "services       265\n",
      "project        263\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rare words and count:\n",
      " assistantsproduction          1\n",
      "departmentsdirectormanager    1\n",
      "taxvice                       1\n",
      "grande                        1\n",
      "responsibilitymanager         1\n",
      "engineersscm                  1\n",
      "paralegal                     1\n",
      "explorationgeophysical        1\n",
      "companymarket                 1\n",
      "engineersrisk                 1\n",
      "dtype: int64 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rushi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rushi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:60: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Embeddings original dimension:  16\n"
     ]
    }
   ],
   "source": [
    "df=df.fillna(\"\")\n",
    "familypos= df[['Department: ', 'Position Title:']].apply(lambda x: ''.join(x), axis=1)\n",
    "print(\"Total Length:\",len(familypos))\n",
    "familypos=familypos.dropna()\n",
    "print(\"Total Length after removing NA:\",len(familypos))\n",
    "print(\"unique Length:\",len(set(familypos)))\n",
    "#correct = []\n",
    "#for i in tqdm(set(familypos)): correct.append(''.join(textblob.TextBlob(i).correct()))\n",
    "#print(\"unique Length after spelling correction:\",len(set(correct)))\n",
    "word_count = familypos.apply(lambda x: len(str(x).split(\" \")))\n",
    "print(\"max word count\",max(word_count))\n",
    "print(\"min word count\",min(word_count))\n",
    "print(\"max word count\",sum(word_count)/len(word_count))\n",
    "print(\"average character count\",max(familypos.str.len()))\n",
    "print(\"min character count\",min(familypos.str.len()))\n",
    "stop_words= familypos.apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "print(\"Total Stop Words\",sum(stop_words))\n",
    "special_character = familypos.apply(lambda x: len([x for x in x.split() if x in ('&','@','#','$','%','!')]))\n",
    "print(\"Total special_character\",sum(special_character))\n",
    "numerics = familypos.apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "print(\"Total numbers\",sum(numerics))\n",
    "upper = familypos.apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "print(\"Total upprfamilyposse\",sum(upper))\n",
    "\n",
    "####\n",
    "familypos = familypos.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "familypos = familypos.str.replace('[^\\w\\s]','')\n",
    "familypos = familypos.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "####\n",
    "\n",
    "cfreq = pd.Series(' '.join(familypos).split()).value_counts()[:10]\n",
    "print(\"\\n\\nCommon words and count:\\n\",cfreq,'\\n\\n')\n",
    "rfreq = pd.Series(' '.join(familypos).split()).value_counts()[-10:]\n",
    "print(\"\\n\\nrare words and count:\\n\",rfreq,'\\n\\n')\n",
    "\n",
    "####\n",
    "familypos=familypos.apply(lambda x: \" \".join(x for x in x.split() if x not in rfreq))\n",
    "#familypos =familypos.apply(lambda x: \" \".join[Word(word).lemmatize() for word in x.split()])\n",
    "####\n",
    "\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(familypos)]\n",
    "\n",
    "max_epochs = 100\n",
    "vec_size = max(word_count)\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    model.alpha -= 0.0002\n",
    "\n",
    "\n",
    "model.save(\"d2v_familypos.model\")\n",
    "\n",
    "model=Doc2Vec.load(\"d2v_familypos.model\")\n",
    "\n",
    "familypos_vec = np.array([model.docvecs[i] for i in range(0, len(model.docvecs))])\n",
    "print(\"Embeddings original dimension: \", familypos_vec.shape[1])\n",
    "\n",
    "ide_familypos = pd.DataFrame(familypos)\n",
    "\n",
    "ide_familypos['familypos_vec']=familypos_vec.tolist()\n",
    "ide_familypos['Department: ']=df['Department: ']\n",
    "#mean_vecs = np.mean(familypos_vec, axis=0)\n",
    "#std_vecs = np.std(familypos_vec, axis=0)\n",
    "#familypos_data = (familypos_vec - mean_vecs) / std_vecs\n",
    "\n",
    "#familyposne = familyposNE(n_componenfamilypos=2, verbose=2, perplexity=30, n_iter=1000, random_state=15, learning_rate=2000, early_exaggeration=100)\n",
    "#familyposne_resulfamilypos = familyposne.fit_transform(familypos_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kmean_familypos_cluster.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "ms = MeanShift()\n",
    "ms.fit(familypos_vec)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "joblib.dump(ms,'kmean_familypos_cluster.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4119,)\n",
      "number of estimated family : 49\n"
     ]
    }
   ],
   "source": [
    "ms = joblib.load('kmean_familypos_cluster.pkl')\n",
    "print(labels.shape)\n",
    "print(\"number of estimated family : %d\" % n_clusters_)\n",
    "\n",
    "\n",
    "ide_familypos['label']= labels.tolist()\n",
    "\n",
    "#print(labels.tolist().index(cluster_centers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4119 entries, 0 to 4118\n",
      "Data columns (total 9 columns):\n",
      "Department:          4119 non-null object\n",
      "Position Title:      4119 non-null object\n",
      "POSITION SUMMARY:    4119 non-null object\n",
      "responsabilities     4119 non-null object\n",
      "EI                   4119 non-null object\n",
      "CA                   4119 non-null object\n",
      "TS                   4119 non-null object\n",
      "label                4119 non-null int64\n",
      "family               4119 non-null object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 289.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "simil={}\n",
    "cfamily={}\n",
    "mfamily={}\n",
    "for i in range(len(cluster_centers)):\n",
    "    simil={}\n",
    "    temp = ide_familypos[ide_familypos['label'] == i]\n",
    "    for j in list(temp.index):\n",
    "        val=cosine(np.asarray(temp['familypos_vec'][j]),cluster_centers[i])\n",
    "        simil[temp['Department: '][j]] = val\n",
    "    fam=list(simil.keys())[list(simil.values()).index(max(simil.values()))]\n",
    "    cfamily[i]= fam\n",
    "    mfamily[fam] = simil\n",
    "\n",
    "cfamily\n",
    "\n",
    "df['label']= ide_familypos['label']\n",
    "df['family']=df['label'].map(cfamily)\n",
    "joblib.dump(cfamily,'families.lisfamilypos')\n",
    "df.to_csv('familypos_gen_family.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Contracts Management',\n",
       " 1: 'Husky Lima Refinery',\n",
       " 2: 'Engineers',\n",
       " 3: 'Midstream & Downstream, US Downstream Accounting',\n",
       " 4: 'Engineers (XP)',\n",
       " 5: 'Downstream Strategy & Business Development (Downstream Commercial)',\n",
       " 6: 'Upstream Environmental Operations',\n",
       " 7: 'Procurement Reps',\n",
       " 8: 'Health and Safety',\n",
       " 9: 'Project Engineering and Turnaround',\n",
       " 10: 'Engineers',\n",
       " 11: 'Field Administrators',\n",
       " 12: 'Assistants',\n",
       " 13: 'Project Delivery ',\n",
       " 14: 'Prince George Refinery - Canadian Downstream',\n",
       " 15: 'Crude Oil Marketing',\n",
       " 16: 'Safety, Engineering & Procurement',\n",
       " 17: 'Commercial & Industrial ',\n",
       " 18: 'Western Canada, Upstream Finance & Accounting',\n",
       " 19: 'Technical Students',\n",
       " 20: 'Non-Technical Students',\n",
       " 21: 'HS&ER - Pipelines and Terminals - Canadian Downstream',\n",
       " 22: 'Refined Products - Fuel Marketing & Ancillary Sales',\n",
       " 23: 'Technical Services Analysts',\n",
       " 24: 'Non-Technical Students',\n",
       " 25: 'Canadian Products Marketing',\n",
       " 26: 'Heavy Oil & Gas, Upstream Finance & Accounting',\n",
       " 27: 'Operations - Pipelines and Terminals - Canadian Downstream',\n",
       " 28: 'Sunrise Maintenance',\n",
       " 29: 'Oil Sands Reservoir Engineering - Sunrise ',\n",
       " 30: 'Maintenance - Pipelines and Terminals - Canadian Downstream',\n",
       " 31: 'Oil Sands Subsurface',\n",
       " 32: 'Commercial & Industrial ',\n",
       " 33: 'Information Services Operations',\n",
       " 34: 'Engineers (US)',\n",
       " 35: 'Geophysics',\n",
       " 36: 'Midstream & Downstream, Infrastructure & Marketing',\n",
       " 37: 'Across Husky',\n",
       " 38: 'Supply & Logistics - Supply Operations',\n",
       " 39: 'Western Canada Exploration',\n",
       " 40: 'Corporate Services',\n",
       " 41: 'Financial Reporting',\n",
       " 42: 'Procurement Reps',\n",
       " 43: 'Western Canada Production, Operations & Projects (WCP O&P)',\n",
       " 44: 'Finance - Controller',\n",
       " 45: 'Corporate Services',\n",
       " 46: 'Information Services',\n",
       " 47: 'Non-Technical Students',\n",
       " 48: 'Information Services'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfamily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(ide_family.info())\n",
    "\n",
    "#ide_family.head()\n",
    "#centers={}\n",
    "#print(str(cluster_centers[0]))\n",
    "#print(family_vec.shape[0])\n",
    "#cluster_centers=cluster_centers.tolist()\n",
    "#for i in range(len(cluster_centers)):\n",
    "#    print(ide_family['family_vec'].isin((cluster_centers[i])))\n",
    "    #center[str(i)] = ide_family['Department: '][ide_family['family_vec'].str == str(cluster_centers[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(family)]\n",
    "\n",
    "\n",
    "#max_epochs = 10 * max(word_count)\n",
    "#vec_size = max(word_count)\n",
    "#alpha = 0.025\n",
    "\n",
    "#model = Doc2Vec(size=vec_size,\n",
    "#                alpha=alpha, \n",
    "#                min_alpha=0.00025,\n",
    "#                min_count=1,\n",
    "#                dm =1)\n",
    "  \n",
    "#model.build_vocab(tagged_data)\n",
    "\n",
    "#for epoch in range(max_epochs):\n",
    "#    print('iteration {0}'.format(epoch))\n",
    "#    model.train(tagged_data,\n",
    "#                total_examples=model.corpus_count,\n",
    "#                epochs=model.iter)\n",
    "#    model.alpha -= 0.0002\n",
    "\n",
    "\n",
    "#model.save(\"d2v_family.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
